"""
Integration tests for end-to-end parsing pipeline.

This test module validates the complete parsing pipeline with real-world
scenarios, including grading pipeline integration and performance validation.
"""

import pytest
import time
import json
from typing import Dict, Any, List
from unittest.mock import Mock, patch, MagicMock

# Import core modules
from core.enhanced_response_parser import EnhancedResponseParser
from core.grading_pipeline import GradingPipeline
from core.parsing_models import ParsingConfig, SuccessLevel
from core.dynamic_models import DynamicModelFactory
from models.llm_manager import LLMManager


class TestEndToEndParsingPipeline:
    """Integration tests for complete parsing pipeline."""
    
    def setup_method(self):
        """Setup test environment with mocked dependencies."""
        # Create test rubric
        self.test_rubric = [
            {
                'main_criterion': 'ì§€ë¦¬ì  ì •í™•ì„±',
                'sub_criteria': [
                    {'score': 30, 'content': 'ìœ„ì¹˜ ì •í™•ì„±'},
                    {'score': 20, 'content': 'ëª…ì¹­ ì •í™•ì„±'}
                ]
            },
            {
                'main_criterion': 'í‘œí˜„ ì™„ì„±ë„',
                'sub_criteria': [
                    {'score': 25, 'content': 'í‘œê¸° ëª…í™•ì„±'},
                    {'score': 25, 'content': 'ì „ì²´ ì™„ì„±ë„'}
                ]
            }
        ]
        
        # Create parser from rubric
        self.parser = DynamicModelFactory.create_parser(self.test_rubric)
        
        # Mock LLM manager
        self.mock_llm_manager = Mock(spec=LLMManager)
        self.mock_llm = Mock()
        self.mock_llm_manager.get_llm.return_value = self.mock_llm
        
        # Mock retriever
        self.mock_retriever = Mock()
        
        # Create grading pipeline
        self.grading_pipeline = GradingPipeline(
            llm_manager=self.mock_llm_manager,
            retriever=self.mock_retriever,
            parsing_config=ParsingConfig(
                max_attempts=4,
                enable_fallback_recovery=True,
                enable_partial_recovery=True,
                log_all_attempts=True
            )
        )
    
    @patch('utils.retrieval.retrieve_documents')
    @patch('utils.retrieval.rerank_documents')
    @patch('prompts.prompt_templates.get_grading_prompt')
    def test_successful_end_to_end_parsing(self, mock_get_prompt, mock_rerank, mock_retrieve):
        """Test successful end-to-end parsing with perfect LLM response."""
        # Setup mocks
        mock_doc = Mock()
        mock_doc.page_content = "Sample document content"
        mock_doc.metadata = {"source": "test.pdf", "page": 1}
        
        mock_retrieve.return_value = [mock_doc]
        mock_rerank.return_value = [mock_doc]
        mock_get_prompt.return_value = "Test grading prompt"
        
        # Perfect LLM response
        perfect_response = """{
            "ì±„ì ê²°ê³¼": {
                "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1_ì ìˆ˜": 45,
                "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_1_1_ì ìˆ˜": 25,
                "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_1_2_ì ìˆ˜": 20,
                "ì£¼ìš”_ì±„ì _ìš”ì†Œ_2_ì ìˆ˜": 50,
                "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_2_1_ì ìˆ˜": 25,
                "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_2_2_ì ìˆ˜": 25,
                "í•©ì‚°_ì ìˆ˜": 95,
                "ì ìˆ˜_íŒë‹¨_ê·¼ê±°": {
                    "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1": "ìœ„ì¹˜ì™€ ëª…ì¹­ì´ ëª¨ë‘ ì •í™•í•¨",
                    "ì£¼ìš”_ì±„ì _ìš”ì†Œ_2": "í‘œê¸°ê°€ ëª…í™•í•˜ê³  ì™„ì„±ë„ê°€ ë†’ìŒ"
                }
            },
            "í”¼ë“œë°±": {
                "êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ì§€ë„ í‘œê¸°ê°€ ë§¤ìš° ì •í™•í•˜ê³  ì™„ì„±ë„ê°€ ë†’ìŠµë‹ˆë‹¤. ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì˜€ìŠµë‹ˆë‹¤.",
                "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false,
                "ì˜ì‚¬_ì‘ë‹µ_ì„¤ëª…": ""
            }
        }"""
        
        self.mock_llm_manager.call_llm_with_retry.return_value = perfect_response
        
        # Process student answer
        result = self.grading_pipeline.process_student_answer(
            student_name="ê¹€ì² ìˆ˜",
            student_answer="í•œêµ­ì˜ ì£¼ìš” ë„ì‹œë“¤ì„ í‘œê¸°í•œ ì§€ë„ì…ë‹ˆë‹¤.",
            rubric=self.test_rubric,
            question_type="ì§€ë„ í‘œê¸°",
            parser=self.parser
        )
        
        # Verify successful processing
        assert "ì˜¤ë¥˜" not in result
        assert result["ì´ë¦„"] == "ê¹€ì² ìˆ˜"
        assert result["ì±„ì ê²°ê³¼"]["í•©ì‚°_ì ìˆ˜"] == 95
        assert "ì§€ë„ í‘œê¸°ê°€ ë§¤ìš° ì •í™•í•˜ê³ " in result["í”¼ë“œë°±"]["êµê³¼_ë‚´ìš©_í”¼ë“œë°±"]
        assert "test.pdf" in result["ì°¸ê³ ë¬¸ì„œ"]
        assert "ì±„ì _ì†Œìš”_ì‹œê°„" in result
    
    @patch('utils.retrieval.retrieve_documents')
    @patch('utils.retrieval.rerank_documents')
    @patch('prompts.prompt_templates.get_grading_prompt')
    def test_end_to_end_with_malformed_response(self, mock_get_prompt, mock_rerank, mock_retrieve):
        """Test end-to-end processing with malformed LLM response that requires fixing."""
        # Setup mocks
        mock_doc = Mock()
        mock_doc.page_content = "Reference content"
        mock_doc.metadata = {"source": "reference.pdf", "page": 2}
        
        mock_retrieve.return_value = [mock_doc]
        mock_rerank.return_value = [mock_doc]
        mock_get_prompt.return_value = "Grading prompt"
        
        # Malformed response that should be recoverable
        malformed_response = """Here is the grading result:
        
        ```json
        {
            "ì±„ì ê²°ê³¼": {
                "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1_ì ìˆ˜": "35",  // String instead of int
                "í•©ì‚°_ì ìˆ˜": 75,
                "ì ìˆ˜_íŒë‹¨_ê·¼ê±°": {
                    "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1": "ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±"
                },
            },  // Trailing comma
            "í”¼ë“œë°±": {
                "êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ê¸°ë³¸ì ì¸ ë‚´ìš©ì€ í¬í•¨ë˜ì–´ ìˆìœ¼ë‚˜ ì„¸ë¶€ì‚¬í•­ ë³´ì™„ í•„ìš”",
                "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": "false"  // String instead of boolean
                // Missing ì˜ì‚¬_ì‘ë‹µ_ì„¤ëª…
            }
        }
        ```
        
        End of grading."""
        
        self.mock_llm_manager.call_llm_with_retry.return_value = malformed_response
        
        # Process student answer
        result = self.grading_pipeline.process_student_answer(
            student_name="ë°•ì˜í¬",
            student_answer="ì§€ë„ì— ì£¼ìš” ë„ì‹œë¥¼ í‘œê¸°í–ˆìŠµë‹ˆë‹¤.",
            rubric=self.test_rubric,
            question_type="ì§€ë„ í‘œê¸°",
            parser=self.parser
        )
        
        # Should succeed with corrections
        assert result["ì´ë¦„"] == "ë°•ì˜í¬"
        assert result["ì±„ì ê²°ê³¼"]["í•©ì‚°_ì ìˆ˜"] == 75
        assert result["ì±„ì ê²°ê³¼"]["ì£¼ìš”_ì±„ì _ìš”ì†Œ_1_ì ìˆ˜"] == 35  # Should be converted to int
        assert result["í”¼ë“œë°±"]["ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€"] == False  # Should be converted to boolean
        
        # Should have parsing warnings
        if "íŒŒì‹±_ê²½ê³ " in result:
            assert len(result["íŒŒì‹±_ê²½ê³ "]) > 0
    
    @patch('utils.retrieval.retrieve_documents')
    @patch('utils.retrieval.rerank_documents')
    @patch('prompts.prompt_templates.get_grading_prompt')
    def test_end_to_end_with_complete_parsing_failure(self, mock_get_prompt, mock_rerank, mock_retrieve):
        """Test end-to-end processing when all parsing strategies fail."""
        # Setup mocks
        mock_doc = Mock()
        mock_doc.page_content = "Reference content"
        mock_doc.metadata = {"source": "doc.pdf", "page": 1}
        
        mock_retrieve.return_value = [mock_doc]
        mock_rerank.return_value = [mock_doc]
        mock_get_prompt.return_value = "Grading prompt"
        
        # Response that will trigger emergency fallback
        unparseable_response = "I cannot provide a proper grading result. The student's work is difficult to evaluate. Score might be around 60 points."
        
        self.mock_llm_manager.call_llm_with_retry.return_value = unparseable_response
        
        # Process student answer
        result = self.grading_pipeline.process_student_answer(
            student_name="ì´ë¯¼ìˆ˜",
            student_answer="ê°„ë‹¨í•œ ì§€ë„ë¥¼ ê·¸ë ¸ìŠµë‹ˆë‹¤.",
            rubric=self.test_rubric,
            question_type="ì§€ë„ í‘œê¸°",
            parser=self.parser
        )
        
        # Should use emergency fallback
        assert result["ì´ë¦„"] == "ì´ë¯¼ìˆ˜"
        assert "ì˜¤ë¥˜" in result and "ë¶€ë¶„ì  íŒŒì‹± ì„±ê³µ" in result["ì˜¤ë¥˜"]
        assert result["ì±„ì ê²°ê³¼"]["í•©ì‚°_ì ìˆ˜"] == 60  # Should extract score from text
        assert "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´" in result["í”¼ë“œë°±"]["êµê³¼_ë‚´ìš©_í”¼ë“œë°±"]
        assert "ì›ë³¸_ì‘ë‹µ_ìƒ˜í”Œ" in result
    
    @patch('utils.retrieval.retrieve_documents')
    @patch('utils.retrieval.rerank_documents')
    @patch('prompts.prompt_templates.get_grading_prompt')
    def test_end_to_end_with_llm_failure(self, mock_get_prompt, mock_rerank, mock_retrieve):
        """Test end-to-end processing when LLM call fails."""
        # Setup mocks
        mock_retrieve.return_value = []
        mock_rerank.return_value = []
        mock_get_prompt.return_value = "Grading prompt"
        
        # LLM returns None (failure)
        self.mock_llm_manager.call_llm_with_retry.return_value = None
        
        # Process student answer
        result = self.grading_pipeline.process_student_answer(
            student_name="ìµœì§€ì€",
            student_answer="ì§€ë„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.",
            rubric=self.test_rubric,
            question_type="ì§€ë„ í‘œê¸°",
            parser=self.parser
        )
        
        # Should handle LLM failure gracefully
        assert result["ì´ë¦„"] == "ìµœì§€ì€"
        assert "ì˜¤ë¥˜" in result
        assert "LLM ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" in result["ì˜¤ë¥˜"]


class TestPerformanceAndReliability:
    """Test performance characteristics and reliability improvements."""
    
    def setup_method(self):
        """Setup performance testing environment."""
        self.config = ParsingConfig(
            max_attempts=4,
            enable_fallback_recovery=True,
            enable_partial_recovery=True,
            log_all_attempts=False  # Reduce logging for performance tests
        )
        self.parser_instance = EnhancedResponseParser(self.config)
        
        # Create test rubric and parser
        self.test_rubric = [
            {
                'main_criterion': 'ê¸°ë³¸ ìš”êµ¬ì‚¬í•­',
                'sub_criteria': [
                    {'score': 50, 'content': 'í•„ìˆ˜ ë‚´ìš© í¬í•¨'}
                ]
            }
        ]
        self.pydantic_parser = DynamicModelFactory.create_parser(self.test_rubric)
    
    def test_parsing_performance_benchmark(self):
        """Test parsing performance with various response types."""
        test_cases = [
            # Perfect JSON
            '{"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 85}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ì¢‹ìŒ", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false}}',
            
            # JSON with artifacts
            '''Here is the result:
            ```json
            {"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 75}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ë³´í†µ", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false}}
            ```''',
            
            # Malformed JSON
            '{"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 65,}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ìˆ˜ì •í•„ìš”", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false,}}',
            
            # Text requiring emergency fallback
            'The score should be around 55 points. Good effort but needs improvement.'
        ]
        
        total_time = 0
        successful_parses = 0
        
        for i, response in enumerate(test_cases):
            start_time = time.time()
            
            result = self.parser_instance.parse_response_with_rubric(
                response, self.pydantic_parser, self.test_rubric
            )
            
            end_time = time.time()
            parse_time = (end_time - start_time) * 1000  # Convert to ms
            total_time += parse_time
            
            # Verify result is usable
            assert result.success_level in [SuccessLevel.FULL, SuccessLevel.PARTIAL]
            assert result.has_usable_data()
            
            if result.success_level == SuccessLevel.FULL:
                successful_parses += 1
            
            # Performance assertion - should complete within reasonable time
            assert parse_time < 1000, f"Parse {i} took too long: {parse_time}ms"
            
            print(f"Test case {i}: {parse_time:.2f}ms, Success level: {result.success_level.value}")
        
        avg_time = total_time / len(test_cases)
        success_rate = successful_parses / len(test_cases) * 100
        
        print(f"Average parsing time: {avg_time:.2f}ms")
        print(f"Full success rate: {success_rate:.1f}%")
        
        # Performance targets
        assert avg_time < 500, f"Average parsing time too high: {avg_time}ms"
        assert success_rate >= 50, f"Success rate too low: {success_rate}%"
    
    def test_fallback_reliability(self):
        """Test that fallback mechanisms always provide usable results."""
        problematic_responses = [
            "",  # Empty response
            "No JSON here at all",  # No structured content
            "{'invalid': json syntax}",  # Invalid JSON syntax
            "ì ìˆ˜ëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",  # Korean text with no extractable score
            "Score: ì ìˆ˜ë¥¼ ë§¤ê¸¸ ìˆ˜ ì—†ìŒ",  # Mixed language, no score
            "The student did well, maybe 80-90 points range",  # Ambiguous score
            "JSON: {broken syntax, no closing brace",  # Broken JSON
            "```\nNot JSON content\n```",  # Code block with non-JSON content
        ]
        
        for i, response in enumerate(problematic_responses):
            result = self.parser_instance.parse_response_with_rubric(
                response, self.pydantic_parser, self.test_rubric
            )
            
            # Even with problematic input, should always get usable data
            assert result.has_usable_data(), f"Response {i} failed to provide usable data"
            
            best_data = result.get_best_data()
            assert best_data is not None
            assert "ì±„ì ê²°ê³¼" in best_data
            assert "í”¼ë“œë°±" in best_data
            assert "í•©ì‚°_ì ìˆ˜" in best_data["ì±„ì ê²°ê³¼"]
            assert "êµê³¼_ë‚´ìš©_í”¼ë“œë°±" in best_data["í”¼ë“œë°±"]
            
            print(f"Problematic response {i}: {result.success_level.value}")
    
    def test_adaptive_validation_improvement(self):
        """Test that adaptive validation improves success rates."""
        # Response with non-standard field structure
        response_with_variations = """{
            "ì±„ì ê²°ê³¼": {
                "ì£¼ìš”ì ìˆ˜_1": 40,  // Different field name pattern
                "total_score": 85,  // English field name
                "reasoning": {"note": "Good work"},  // Different structure
                "extra_field": "ignored"  // Extra field
            },
            "í”¼ë“œë°±": {
                "content_feedback": "í•™ìƒì´ ì˜ í–ˆìŠµë‹ˆë‹¤",  // English field name
                "is_bluffing": false,  // English field name
                "explanation": "Solid work"
            }
        }"""
        
        # Test with standard validation (should be more strict)
        result_standard = self.parser_instance.parse_response(response_with_variations, self.pydantic_parser)
        
        # Test with adaptive validation (should be more flexible)
        result_adaptive = self.parser_instance.parse_response_with_rubric(
            response_with_variations, self.pydantic_parser, self.test_rubric
        )
        
        # Adaptive validation should be more successful or equal
        assert result_adaptive.success_level.value >= result_standard.success_level.value
        
        # At minimum, adaptive validation should provide usable data
        assert result_adaptive.has_usable_data()
        
        print(f"Standard validation: {result_standard.success_level.value}")
        print(f"Adaptive validation: {result_adaptive.success_level.value}")


class TestRealWorldScenarios:
    """Test with realistic, complex scenarios that might occur in production."""
    
    def setup_method(self):
        """Setup realistic testing environment."""
        # Complex rubric similar to real geography grading
        self.geography_rubric = [
            {
                'main_criterion': 'ì§€ë¦¬ì  ìœ„ì¹˜ ì •í™•ì„±',
                'sub_criteria': [
                    {'score': 15, 'content': 'ì£¼ìš” ë„ì‹œ ìœ„ì¹˜'},
                    {'score': 15, 'content': 'í–‰ì •êµ¬ì—­ ê²½ê³„'},
                    {'score': 10, 'content': 'ì§€í˜•ì  íŠ¹ì§•'}
                ]
            },
            {
                'main_criterion': 'êµí†µ ë° ì¸í”„ë¼',
                'sub_criteria': [
                    {'score': 20, 'content': 'ì£¼ìš” êµí†µë¡œ'},
                    {'score': 10, 'content': 'ê³µí•­ ë° í•­ë§Œ'},
                    {'score': 10, 'content': 'ì² ë„ ë„¤íŠ¸ì›Œí¬'}
                ]
            },
            {
                'main_criterion': 'í‘œê¸° ì™„ì„±ë„',
                'sub_criteria': [
                    {'score': 10, 'content': 'ëª…ì¹­ ì •í™•ì„±'},
                    {'score': 10, 'content': 'ì „ì²´ì  ì™„ì„±ë„'}
                ]
            }
        ]
        
        self.parser = DynamicModelFactory.create_parser(self.geography_rubric)
        self.config = ParsingConfig(
            max_attempts=4,
            enable_fallback_recovery=True,
            enable_partial_recovery=True,
            log_all_attempts=True
        )
        self.enhanced_parser = EnhancedResponseParser(self.config)
    
    def test_realistic_llm_response_variations(self):
        """Test with realistic variations of LLM responses."""
        realistic_responses = [
            # Verbose response with explanation
            """í•™ìƒì˜ ì§€ë„ë¥¼ í‰ê°€í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```json
{
    "ì±„ì ê²°ê³¼": {
        "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1_ì ìˆ˜": 35,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_1_1_ì ìˆ˜": 12,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_1_2_ì ìˆ˜": 13,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_1_3_ì ìˆ˜": 10,
        "ì£¼ìš”_ì±„ì _ìš”ì†Œ_2_ì ìˆ˜": 45,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_2_1_ì ìˆ˜": 20,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_2_2_ì ìˆ˜": 15,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_2_3_ì ìˆ˜": 10,
        "ì£¼ìš”_ì±„ì _ìš”ì†Œ_3_ì ìˆ˜": 18,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_3_1_ì ìˆ˜": 8,
        "ì„¸ë¶€_ì±„ì _ìš”ì†Œ_3_2_ì ìˆ˜": 10,
        "í•©ì‚°_ì ìˆ˜": 98,
        "ì ìˆ˜_íŒë‹¨_ê·¼ê±°": {
            "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1": "ì§€ë¦¬ì  ìœ„ì¹˜ê°€ ëŒ€ì²´ë¡œ ì •í™•í•˜ë‚˜ ì¼ë¶€ ìˆ˜ì • í•„ìš”",
            "ì£¼ìš”_ì±„ì _ìš”ì†Œ_2": "êµí†µ ì¸í”„ë¼ê°€ ì˜ í‘œí˜„ë˜ì–´ ìˆìŒ",
            "ì£¼ìš”_ì±„ì _ìš”ì†Œ_3": "í‘œê¸°ê°€ ëª…í™•í•˜ê³  ì™„ì„±ë„ê°€ ë†’ìŒ"
        }
    },
    "í”¼ë“œë°±": {
        "êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì§€ë„ì…ë‹ˆë‹¤. ì£¼ìš” ë„ì‹œì™€ êµí†µë§ì´ ì •í™•í•˜ê²Œ í‘œê¸°ë˜ì–´ ìˆê³ , í–‰ì •êµ¬ì—­ë„ ì ì ˆíˆ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì¼ë¶€ ì§€í˜•ì  íŠ¹ì§•ì˜ í‘œê¸°ê°€ ë¯¸í¡í•œ ë¶€ë¶„ì´ ìˆì–´ ë³´ì™„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false,
        "ì˜ì‚¬_ì‘ë‹µ_ì„¤ëª…": ""
    }
}
```

ì´ìƒìœ¼ë¡œ ì±„ì ì„ ì™„ë£Œí•©ë‹ˆë‹¤.""",
            
            # Response with mixed formatting
            """ì±„ì  ê²°ê³¼:
            
{
"ì±„ì ê²°ê³¼": {
"ì£¼ìš”_ì±„ì _ìš”ì†Œ_1_ì ìˆ˜": 25,
"í•©ì‚°_ì ìˆ˜": 75,
"ì ìˆ˜_íŒë‹¨_ê·¼ê±°": {
"overall": "ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ì€ ì¶©ì¡±í•˜ì˜€ìœ¼ë‚˜ ì„¸ë¶€ì‚¬í•­ ë³´ì™„ í•„ìš”"
}
},
"í”¼ë“œë°±": {
"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ì§€ë„ì˜ ê¸°ë³¸ êµ¬ì¡°ëŠ” ì˜ ì¡í˜€ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
"ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false
}
}""",
            
            # Response with errors but extractable content
            """{
    "ì±„ì ê²°ê³¼": {
        "ì£¼ìš”_ì±„ì _ìš”ì†Œ_1_ì ìˆ˜": "30",  # String instead of int
        "í•©ì‚°_ì ìˆ˜": 65,
        "ì ìˆ˜_íŒë‹¨_ê·¼ê±°": {"note": "Acceptable performance"},
    },  # Trailing comma
    "í”¼ë“œë°±": {
        "êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "í•™ìƒì´ ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.",
        "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": "false",  # String instead of boolean
        # Missing ì˜ì‚¬_ì‘ë‹µ_ì„¤ëª…
    }
}"""
        ]
        
        for i, response in enumerate(realistic_responses):
            result = self.enhanced_parser.parse_response_with_rubric(
                response, self.parser, self.geography_rubric
            )
            
            # Should successfully parse or provide fallback
            assert result.has_usable_data(), f"Failed to parse realistic response {i}"
            
            best_data = result.get_best_data()
            assert "ì±„ì ê²°ê³¼" in best_data
            assert "í”¼ë“œë°±" in best_data
            assert "í•©ì‚°_ì ìˆ˜" in best_data["ì±„ì ê²°ê³¼"]
            
            print(f"Realistic response {i}: {result.success_level.value}")
            if result.warnings:
                print(f"  Warnings: {len(result.warnings)}")
    
    def test_edge_case_scenarios(self):
        """Test edge cases that might occur in production."""
        edge_cases = [
            # Very long response
            '{"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 80}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "' + "ë§¤ìš° " * 1000 + 'ì¢‹ìŠµë‹ˆë‹¤", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false}}',
            
            # Response with special characters
            '{"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 70}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "å­¦ç”Ÿä½œå“ä¸é”™ï¼Good work~ ğŸŒŸ", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false}}',
            
            # Response with nested Korean content
            '{"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 90, "ìƒì„¸ë‚´ìš©": {"í‰ê°€": "ìš°ìˆ˜í•¨", "ê°œì„ ì ": "ì—†ìŒ"}}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ì™„ë²½í•©ë‹ˆë‹¤", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false}}',
            
            # Response with encoding issues simulation
            '{"ì±„ì ê²°ê³¼": {"í•©ì‚°_ì ìˆ˜": 60}, "í”¼ë“œë°±": {"êµê³¼_ë‚´ìš©_í”¼ë“œë°±": "ê¸°ë³¸ì ì¸ ë‚´ìš© í¬í•¨", "ì˜ì‚¬_ì‘ë‹µ_ì—¬ë¶€": false}}',
        ]
        
        for i, response in enumerate(edge_cases):
            try:
                result = self.enhanced_parser.parse_response_with_rubric(
                    response, self.parser, self.geography_rubric
                )
                
                # Should handle edge cases gracefully
                assert result.has_usable_data(), f"Failed to handle edge case {i}"
                print(f"Edge case {i}: {result.success_level.value}")
                
            except Exception as e:
                pytest.fail(f"Edge case {i} caused exception: {e}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])